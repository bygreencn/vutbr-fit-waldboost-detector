%=========================================================================
% (c) Michal Bidlo, Bohuslav Køena, 2008

\chapter{GPGPU}

With high demand for real-time image processing, computer vision applications and a need for fast calculations in the scientific world, general-purpose computing on graphics processor units, also known as the GPGPU, has become a popular programming model to accelerate programs traditionally coded on the CPU (Central Processing Unit) using the data-parallel processing powers of the GPU.

Until the last decade or so, when technologies for GPGPU became available, the GPU was used mostly to render data given to it by the CPU. This has changed in a way, that the GPU, with its massive parallel capabilities, isn't used only for displaying, but also for computation. The traditional approach is to transfer data bidirectionally between the CPU and the GPU, which on one hand brings the overhead of copying the data, but on the other enables to do the calculations many times faster due to the architecture of the GPU. As shown on \ref{fig:cpu-gpu} many more transistors are dedicated to data processing instead of cache or control, which leads to a higher memory bandwidth.

\begin{center}
\begin{figure}[h]
	\centering\includegraphics[height=5cm]{fig/cpu-gpu.png}
	\caption{CPU and GPU architecture comparison (\cite{cuda-toolkit-docs})}
	\label{fig:cpu-gpu}
\end{figure}
\end{center}

GPUs are also designed with demand for floating-point capabilities in mind, which can be taken advantage of in applications such as object detection, where most of the math is done in single-point arithmetic.

\begin{center}
\begin{figure}[h]
	\centering\includegraphics[height=10cm]{fig/floating-point-operations-per-second.png}
	\caption{Floating-Point operations per second for the CPU and GPU (\cite{cuda-toolkit-docs})}
\end{figure}
\end{center}

\section{Parallel computing platforms}

In November 2006 the first parallel computing platform - CUDA (Compute Unified Device Architecture) was introduced by NVIDIA. Since then several others were created by other vendors:

\begin{itemize}
	\item CUDA - NVIDIA
	\item OpenCL - Khronos Group
	\item C++ AMP - Microsoft	
	\item Compute shaders - OpenGL
	\item DirectCompute - Microsoft
\end{itemize}

All of the technologies above allow access to the GPU computing capabilities. The first two - CUDA and OpenCL work on a kernel basis. As a programmer you have access to low-level GPU capabilities and have to manage all the resources yourself. The standard approach is the following:

\begin{enumerate}
	\item Allocate memory on the GPU
	\item Copy data from the CPU to the allocated memory on the GPU
	\item Run a GPU based kernel (written in CUDA or OpenCL)
	\item Copy processed data back from the GPU to the CPU
\end{enumerate}

C++ AMP is a more higher-level oriented library. Introduced by Microsoft as a new C++ feature for Visual Studio 2012 with STL-like syntax, it is designed to accelerate code using massive parallelism. Currently it is supported by most GPUs, which have a DirectX 11 driver.

The last two - Compute shaders and DirectCompute also work in a more high-level fashion, but also quite differently from C++ AMP. They are not a part of the rendering pipeline, but can be set to be executed among other OpenGL or DirectX shaders. The difference between compute shaders and other shaders is, that they don't have specified input or output. These must be specified by the programmer. Theoretically it is then possible to write the whole rendering pipeline using compute shaders only.

\section{NVIDIA CUDA}

NVIDIA CUDA is a programming model enabling direct access to the instruction set and memory of NVIDIA GPUs.

\subsection{Programming model}

CUDA C extends C and uses NVCC compiler to generate code for the GPU. It also allows to write C-like functions called kernels. A kernel is defined by the \verb|__global__| declaration specifier and executed using a given configuration wrapped in \verb|<<< ... >>>|. The configuration is called a grid and takes as parameters the number of blocks and the number of threads. The same kernel code is run by the whole grid. Also code run by the kernel is called to device code, where as the code run outside of the kernel is called the host code.

\paragraph{Threads} are a basic computational unit identified by a 3-dimensional id \verb|threadIdx|, which is typically used to index arrays.

\paragraph{Blocks} are groups of threads, where each block resides on a single processor core, therefore a kernel can be run with the maximum of 1024 threads.

\begin{center}
\begin{figure}[h]
	\centering\includegraphics[height=5.5cm]{fig/grid-of-thread-blocks.png}
	\caption{A grid of blocks and threads run by a kernel (\cite{cuda-toolkit-docs})}
\end{figure}
\end{center}

Kernel configuration parameters can be passed as integers or \verb|dim3| structures. \verb|dim3| specifies the number of threads or blocks in every dimension, therefore a \verb|dim3 threadsPerBlock(4,4,1)| would run a kernel with 16 threads per block, where \verb|threadIdx.x| would range between 0 and 3 and the same for \verb|threadIdx.y|.

Example \ref{code:cuda-example} shows how to add 2 arrays in parallel using N threads and 1 block.

\begin{figure}[h]
\begin{verbatim}
// Kernel definition
__global__ void VecAdd(float* A, float* B, float* C)
{
    int i = threadIdx.x;
    C[i] = A[i] + B[i];
}

int main()
{
    ...
    // Kernel invocation with N threads
    VecAdd<<<1, N>>>(A, B, C);
    ...
}
\end{verbatim}

\caption{Example of vector addition in CUDA (\cite{cuda-toolkit-docs})}
\label{code:cuda-example}
\end{figure}

\FloatBarrier
\subsection{Memory model}\label{subsec:memory}

CUDA threads may access the following types of memories:

\begin{table}
\centering
\begin{tabular}{| l | l | l | l | l |}
\hline
Memory & Keyword & Scope & Access & Lifetime \\
\hline
Registers & - & Thread & Read/Write & Kernel \\
\hline
Local memory & - & Thread & Read/Write & Kernel \\
\hline
Shared memory & \verb|__shared__| & Block & Read/Write &  Kernel \\
\hline
Global memory & \verb|__device__| & Grid & Read/Write & Application \\
\hline
Texture memory & - & Grid & Read-only & Application \\
\hline
Constant memory & \verb|__constant__| & Grid & Read-only & Application \\
\hline
\end{tabular}
\caption{Memory types}
\end{table}

\paragraph{Global memory} is accessible by all threads in a grid and allows read-write. It is also the slowest memory type. Its access is the bottleneck for most applications with access latency ranging from 400 to 800 cycles. There are several strategies for it to be fast like coalescing access with 32B, 64B, 128B transactions.

\paragraph{Texture memory} can be regarded similarly to global memory. Cache is optimized for 2D spatial access pattern and address modes or interpolation can be used at no additional cost.

\paragraph{Constant memory} is the third memory type, which can be accessed by all threads and is typically used to store constants or kernel arguments. It doesn't bring any speed-up compared to global or texture memory, but it is optimized for broadcast.

\paragraph{Shared memory} can be accessed by all threads within a block. It is much faster than the other types, but is subject to bank conflicts.

\paragraph{Unified memory} is a memory type introduced in CUDA 6.0. It enables to use the same memory addresses both in host and device code, which simplifies writing code. On the other as of spring 2014, there doesn't seem to be any hardware support \cite{unified-memory} and the unified memory performs very similar to global memory.

\paragraph{Local memory} is a part of global memory, where everything which doesn't fit into registers is stored. For devices with Compute Capability 2.x there are 32768 32-bit registers.

\begin{center}
\begin{figure}[h]
	\centering\includegraphics[height=12cm]{fig/memory-hierarchy.png}
	\caption{Memory hierarchy (\cite{cuda-toolkit-docs})}
\end{figure}
\end{center}

\chapter{Object detection}

\section{Introduction}

Object detection is a computer technology with the capability of localizing an object in input image data. The type of object depends on which data the detector was trained. Typical applications are human faces, pedestrians, cars, traffic signs and others.

Detector used by the implementation is a frontal-face human detector, which tries to identify a human face within an image. The implementation is therefore optimized for human faces, which means, that the software can be used with other detectors, but it might effect its performance due to specific optimizations. Combined with the capabilities of a GPU, the aim is to produce an object detector capable of real-time object detection on videos.

\section{Features}

There are several methods how to access the topic of object detection. In the following sections we will discuss feature-based object detection.

Let's take a frontal human face as an example. Despite the differences such as lighting, color of eyes or skin, the length of hair, we as humans, can identify we are looking at a human face based on similarities, for example - a pair of eyes, a nose, a pair of ears and so on. These similarities can be called features, but to a computer, they are still too abstract and cannot be enumerated.

\subsection{Local Binary Pattern}

One of the feature methods to describe an image are local binary patterns (LBP). They are based on encoding local intensities of an image with 8-bit codes. In their elementary form they take a 3x3 area as an input and compare intensity values of all the pixels with the central one.

\[
 compare(p_{middle},p_{i}) =
  \begin{cases}
   1 & \text{if } p_{i} \geq p_{middle} \\
   0 & \text{else}
  \end{cases}
\]

LBP value is then evaluated as follows:

\begin{equation}
lbp(p_{middle})=\sum_{i=0}^{7} 2^{i}
compare(p_{middle},p_{i})
\end{equation}


\begin{center}
\begin{figure}[h]
	\centering\includegraphics[width=12cm]{fig/lbp.eps}
	\caption{LBP feature}
\end{figure}
\end{center}

LBPs can be extended to be used not only for single pixels and thus 3x3 areas, but larger for larger ones. For example, when you compare 2x2 areas instead of single pixels, you compare the sum of a middle 2x2 area with the surrounding 2x2 areas.

LBP features are invariant to lighting changes, because even though the image is lighter or darker, the intensity differences stay the same. On the other hand they are not invariant to geometrical transformations such as scale or rotation.

\section{Waldboost}
\label{sec:waldboost}

Only one feature to describe a face is not enough, so a meta-algorithm to process a series of such weak classifiers is needed.

One such algorithm is WaldBoost, which combines AdaBoost and Wald's Sequential Propability Ratio Test (SPRT). SPRT is a strategy to determine what class a sample belongs to, based on a series of measurements.

\[
 SPRT =
  \begin{cases}
   +1 & \text{if } R_{m} \leq B \\
   -1 & \text{if } R_{m} \geq A \\
   \# & \text{else take another measurement} 
  \end{cases}
\]

$R_{m}$ is the likelihood ratio and A, B are constants to compute the wanted false negatives $\alpha$ and false positives $\beta$ ratios as follows:

\begin{equation}
R_{m}=\frac{p(x_{1}, ..., x_{m}|y=-1)}{p(x_{1}, ..., x_{m}|y=+1)}
\end{equation}

\begin{equation}
A=\frac{1-\beta}{\alpha}, B=\frac{\beta}{1-\alpha}
\end{equation}

As mentioned in \cite{sochman-matas-waldboost} with face detection in mind, the positive rate $\beta$ can be set to 0 and the required false negative rate $\alpha$ to a small constant. As such the equations can be simplified to

\begin{equation}
A=\frac{1-0}{\alpha}=\frac{1}{\alpha}, B=\frac{0}{1-\alpha}=0
\end{equation}

and the whole strategy to

\[
 SPRT =
  \begin{cases}
   +1 & \text{if } R_{m} \leq 0 \\
   -1 & \text{if } R_{m} \geq \frac{1}{\alpha} \\
   \# & \text{else take another measurement} 
  \end{cases}
\]

$R_{m}$ is always positive and therefore the algorithm will only classify the sample as a face when it finishes its training cycle or discard it as a background when the ratio gets greater than the given constant A.

\chapter{Implementation}

Project is implemented in \verb|C++| with dependencies on OpenCV - an open source computer vision library with \verb|C| and \verb|C++| interfaces and CUDA - a library for writing NVIDIA GPU code with a CUDA C interface, which is an extension to C. OpenCV is used to load and process separate video frames. It is also designed with a library in mind.

A CMake project is included, therefore it can be built on multiple platforms. The project itself is logically separated into the following modules and sub-modules (in code this is done using \verb|C++| namespaces):

\begin{itemize}
	\item simple - a simple \verb|C++| implementation
	\item gpu - a GPU implementation using CUDA
	\begin{itemize}
		\item pyramid - pyramidal image generation
		\item detection - detection processing
	\end{itemize}
	\item waldboost detector - a class wrapper for simple detector usage
\end{itemize}


\begin{center}
\begin{figure}[h]
	\centering\includegraphics[width=0.8\textwidth]{fig/sample.jpg}
	\caption{A sample output}
\end{figure}
\end{center}

\section{Program structure}

The basic outline of the application pipeline can be described by \ref{fig:pipeline}.

\begin{center}
\begin{figure}[h!]
	\centering\includegraphics[width=9cm]{fig/pipeline.eps}
	\caption{Application pipeline}
	\label{fig:pipeline}	
\end{figure}
\end{center}

\subsection{CUDA initialization}

In this phase all the constants and the detector itself are copied to the GPU. Constants account for data like image width and height, classifier width and height, $\alpha$ count, stage count and so on. The $\alpha$ coefficients and stages of the detector as described in \ref{sec:waldboost} are stored in separate header files generated from an XML file.

\subsection{Kernels}

After loading a video frame and copying the frame to the GPU there are 3 types of kernels to be run.

\begin{itemize}
	\item Preprocessing
	\item Pyramidal image
	\item Detection
\end{itemize}

\subsection{Preprocessing} \label{subsubsec:grayscale} 

First the initial loaded image has to be preprocessed. There are 2 operations to be done - conversion from the initial integers to a float-point representation and then a conversion to grayscale.

Floats are needed in order to work with textures. Texture memory enables hardware implemented bilinear interpolation for subsampling images and this is done many times while creating the pyramidal image.

Conversion to grayscale is a simple image processing operation described by the formula \eqref{eq:rgbtograyscale}. The detector itself is trained on grayscale images, and so the input must also be in grayscale. After the kernel finishes, the result is saved as a texture.

\begin{equation} \label{eq:rgbtograyscale}
Y=0.2126R + 0.7152G + 0.0722B
\end{equation}

\subsection{Dynamic texture and texture objects}

As of CUDA 5.0 and Kepler GPUs textures don't have to be defined globally as static textures, but they can be used dynamically using Texture Objects (\verb|cudaTextureObject_t| class API \cite{cuda-texture-obj}).

This has several advantages. One of them being the slight overhead (up to 1.5 \mu s) by binding and unbinding static textures during kernel launch, which is eliminated. Even though this is not our case and doesn't sound as much, it might be quite a significant overhead while launching large quantities of fast kernels.

The main advantage in our case is, that texture objects can be passed as arguments and therefore used as a part of a library. Also multiple textures can be created using given parameters, which is exploited in pyramidal image creation and explained in \ref{subsec:pyramidal}.

\subsection{Pyramidal image}\label{subsec:pyramidal}

All the pixels are processed using a 26x26 pixel-wide window. The size again depends on how the detector is trained. The basic idea is that all the features inside the window somehow describe an object of similar size as the window. Objects such as faces are usually much larger, therefore we have to create many sub-sampled images and hope, that we find at least one among them, where the object fits inside the window.

In order to do this, the straight forward method is to create a pyramidal image.

As mentioned previously, the implementation uses hardware bilinear interpolation provided by the texture memory for sub-sampling the image. It has to be kept in mind, that bilinear interpolation has some negative side-effects, one of them being the sub-sampling of an image below half its original width.

\begin{center}
\begin{figure}[h]
	\centering\includegraphics[height=9cm]{fig/bilinear_error.pdf}\label{fig:bilinear-error}
	\caption{Error when sub-sampling an image below twice the original width using bilinear interpolation}
\end{figure}
\end{center}

As shown on \ref{fig:bilinear-error} when sub-sampling an image below half its original width, pixels are left out and a sampling error is created.

The image is generated in octaves. An octave is a structure of several images, where the smallest image has half the width/height of the original image. Depending on the number of images in an octave, every image is $2^1/number_of_images$ smaller than the previous. The following octave is then sub-sampled from the previous octave.

Two methods were tested. The first one using a single texture for both writing a reading the individual octaves. This proved to be very costly and therefore another method had been implemented and is described below:

\begin{enumerate}
	\item A pyramidal image of $N$ images is generated, where each image is $2^1/N$ smaller then the previous. Every image is sub-sampled from the original image.
	\item Generated pyramidal image is stored as a dynamic texture. Simultaneously the pyramid is being written inside a final image used for detection.
	\item A pyramidal image is generated, by sub-sampling the previously generated pyramid. Width and height of the sub-sampled pyramid are twice smaller than that of the original.
	\item Generated pyramidal image is saved as a dynamic texture.
	\item Steps 3 and 4 get repeated for a set number of octaves.
\end{enumerate}

\begin{center}
\begin{figure}[h]
	\centering\includegraphics[height=9cm]{fig/pyramid.jpg}
	\caption{Pyramidal image}
\end{figure}
\end{center}

The implementation itself uses 8 levels and 4 octaves. This results in the smallest image being 16x smaller than the original. By reducing the number of octaves or levels, better performance can be gained, but it will be get reflected in the number of detections.

\subsection{Detection}

A Waldboost detector consists of several parts. First we will discuss the general idea of a waldboost detector and the memory organization of the different structures. Then we will focus more on the details of the implementation.

\begin{itemize}
	\item $\alpha$ coefficient table
	\item stages
	\item final threshold
\end{itemize}

To detect an object, the detector has to successfully evaluate a given number of stages, which are processed sequentially. In our case 1024, this number can be reduced, but it will have an impact on the number of detections. Every stage the algorithm processes, a response is given by the $\alpha$-table and sample's LBP. It is then added to the accumulated response. The sample is discarded in case the accumulated value falls below the threshold \verb|thetaB|.

\begin{figure}[h!] \label{fig:stage}
\begin{verbatim}
struct Stage {
    uint8 x, y; // X, Y offsets
    uint8 width, height; // width, height of the feature
    float thetaB; // threshold
    uint32 alphaOffset; // alpha table offset
};
\end{verbatim}
\caption{Stage structure}
\end{figure}

The detector uses a 26x26 pixel-wide window, where \verb|x| and \verb|y| are offsets inside the window and \verb|width| and \verb|height| describe the size of the feature. \verb|alphaOffset| is an offset inside the $\alpha$-table corresponding to appropriate LBP values. \verb|thetaB| is the accumulated threshold value.

\begin{algorithm} \label{alg:detection}
\For{every pixel (a GPU thread is created)}
{
	\For{every stage}
	{
		1. compute LBP coefficient \\
		2. add response for the given LBP to the accumulated response \\
		\If{accumulated response $\geq$ stage threshold $thetaB$}{
			discard sample
		}
	}
}

\caption{Object detection algorithm simplified}
\end{algorithm}

\section{Memory organization} \label{sec:memory-organization}

The use of GPU memory is one of the most important parts of programming on GPU architectures. The types of CUDA memories are described in \ref{subsec:memory}.

Below we will discuss, how the most important parts of the detector are stored and why.

\begin{itemize}
\item \textbf{Stages} - constant memory \\
Stages are stored in the constant memory. Even though it's not as fast as let's say shared memory, its capability to broadcast simultaneously accessed data is ideal. Every thread processes a single image position, for which it loops through a for-cycle of stages. Every read from the constant memory is then not only broadcast to a half-warp (a group of 16 threads), but also cached. The only problem can be the size, which is limited to 64 KB. The detector uses 2048 stages, where each stage is 12 B. This leads to 24 KB, which is enough, but has to be accounted for when storing other data in the constant memory.

\item \textbf{$\alpha$-table} - texture memory \\
Texture memory not only has read-only properties, but also there are 256 coefficients for every stage. Every coefficient is stored as a float, which leads to $256 * 2048 * 4 = 2 MB$ and by far exceeds the memory available for constant memory. Also the access is random, because we are likely to get different LBPs for every pixel.

\item \textbf{Original image and pyramidal image} - texture memory \\
Both are stored in the texture memory. Original image is used to create a pyramidal image using hardware accelerated bilinear interpolation for creating down-sampled images as described in \ref{subsec:pyramidal}. It is highly optimezd for random read-only access, which is exactly what we want.
\end{itemize}

\section{Thread allocation}

A GPU thread is allocated for every sample, therefore every pixel. Based on \cite{herout-realtime-cuda} only a fraction of samples (around 1\%) is still processed by the classifier after only 10 stages. On the other hand the GPU organizes threads in warps - groups of 32 threads, which are organized in blocks across with they can be synchronized. The problem is that, when only a single thread within a warp is active, the other threads have to wait for it until it finishes thus wasting GPU resources.

\cite{herout-realtime-cuda} proposed, that every few stages threads can be checked if they are still evaluating the classifier stages or they have been dropped. Surviving threads can then be reorganized to continue within fewer warps and the other resources to be freed.

The detector uses several ways of reorganizing surviving threads. They are summarized below by the types of functions they use and where surviving threads are stored:

\begin{itemize}
	\item atomic functions / global memory
	\item atomic functions / shared memory
	\item prefix sum / global memory
\end{itemize}

It also uses 3 functions within a kernel to distribute the computation between stages:

\begin{itemize}
	\item \verb|detectSurvivorsInit| - processes all samples for a given number of stages and outputs the surviving threads. Called at the beginning.
	\item \verb|detectSurvivors| - processes surviving samples for a given number of stages and outputs the surviving threads. Called multiple times.
	\item \verb|detectDetections| - processes remaining surviving threads and outputs detections. Called at the end.
\end{itemize}

\subsection{Atomic functions and global memory} \label{subsec:afgm}

The first proposed method uses \verb|atomicInc| on a counter stored in global memory to count the number of surviving threads within a block and global memory for storing the surviving threads. After processing the given number of stages a thread, if it survives, is assigned an ID by atomically incrementing the counter. It also saves the information about the sample it processes in \ref{fig:survivordata} and is written to the global memory on position given by the ID.

\begin{figure}[h!] \label{fig:survivordata}
\begin{verbatim}
struct SurvivorData {
    uint32 x, y; // X, Y coordinates of the sample
    float response; // current response
};
\end{verbatim}
\caption{Stage structure}
\end{figure}

After all running threads reach the given stage \verb|detectSurvivors| or \verb|detectDetections| is run. Surviving samples are packed into fewer warps and lots of GPU resources is freed.

Another way of rearranging the threads was tested by using a .

\subsection{Atomic functions and shared memory}

The second method is very much the same as \ref{subsec:afgm} except it uses shared memory instead of global memory and a counter stored is shared memory. Global memory isn't used at all. The advantage of this method is that shared memory is much faster than global memory (the bandwidth of shared memory is ~1.7TB/s compared to that of global memory which is ~150GB/s).

Shared memory on the other hand only works for a block of threads and so, when packing threads, they are always packed at the start of the shared memory using which the detection is rerun. This has the following advantages and disadvantages:

\subsubsection{Advantages}

\begin{itemize}
	\item Shared memory is 10-12x faster than global memory
	\item Atomic increments are divided between shared memory counters instead of a single global memory counter	
	\item Threads can be synchronized using a \verb|__syncthreads| command, on the other on the grid-level the whole kernel has to be rerun
\end{itemize}

\subsubsection{Disadvantages}

\begin{itemize}
	\item More warps are being run due to threads being packed at the start of the shared memory
\end{itemize}

For example if there would be only 32 surviving threads, the global memory method would only need a single warp. On the other hand the shared memory variant would need as many warps as there are blocks in which the surviving threads reside.


\begin{itemize}

\end{itemize}

\chapter{Results}

\section{Summary}

As of \today ~the detector contains a working GPU and CPU implementations. The CPU version is available for comparison measurements and the GPU version is unoptimized with only a few GPU acceleration features. The latest version is available at: \url{https://github.com/mmaci/vutbr-fit-object-detection}.

\begin{itemize}
	\item Memory usage is likely to stay similar, as there aren't many more viable options. The only other option is unified memory in CUDA 6.0, which seems to be more of a programmer convenient, than performance feature and shared memory, which might be used for local optimizations.
	\item Bilinear interpolation using texture memory is used for image down-sampling, instead of a software implementation.
	\item LBP for 2x1, 1x2 and 2x2 features is calculated using texture memory bilinear interpolation, which leads to a speed-up due to the fact, that sum of the intensity values isn't needed and an average is used instead.
\end{itemize}

\section{Future work}

Some of the ideas and key features yet to be implemented are:

\begin{itemize}
	\item It is generally known, that most of the samples get discarded by the WaldBoost algorithm at the beginning as background. This leads to a large number of threads waiting for the few ones, that still compute. A measurement has to be taken to statistically determine the waiting-thread count and rearrange threads in a way to increase the percentage of running threads.
	\item Other methods of interpolation, such as Lanczos interpolation should be explored and measured compared to the current bilinear interpolation.
	\item The success rate and performance of the detector is also highly dependent on the pyramid image build, therefore other ways to build an optimized pyramid should be explored or if mipmaps can be used instead and thus the whole software based interpolation omitted.
	\item The CPU version should exactly match the algorithm used for the GPU version and also be optimized to provide a valid comparison.

\end{itemize}

%=========================================================================

\nocite{zemcik-high-performance}
\nocite{herout-realtime-cuda}